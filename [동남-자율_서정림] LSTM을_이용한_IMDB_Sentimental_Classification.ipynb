{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60h6dxmms6Gv"
      },
      "source": [
        "# Title: **장단기메모리(Long Short Term Memory;LSTM)**를 이용한 **Sentimental Classification**\n",
        " \n",
        "<img src=\"https://cfml.se/img/blog/sentiment_classification/top_img.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1KiZERXs6Gx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2kk0YYa3DT0"
      },
      "source": [
        " - `tensorflow.keras.layers (layers)`: 딥러닝 네트워크를 설계할 때 층(layer) 관련 함수들(예:`Dense`, `Conv2D`, `MaxPooling2D`, `SimpleRNN`, `LSTM`)을 모아놓은 라이브러리\n",
        "\n",
        " - `tensorflow.keras.models.Model (Model)`: 생성한 층들을 연합하여 하나의 모델로 구성할 때 사용하는 함수\n",
        "\n",
        " - `tensorflow.keras.datasets`: TensorFlow에서 딥러닝 실습을 위해 제공해주는 데이터셋 (예: `mnist`, `cifar10`, `cifar100`, `imdb`)\n",
        "\n",
        " - `numpy (np)`: 다차원 데이터 처리를 위한 라이브러리 (참고: `pandas`-2차원 데이터에 특화된 라이브러리)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sLPxp0Zs6Gy"
      },
      "source": [
        "## Step 1-1) 데이터 불러오기\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/1932/1*fNXlzk-u7VrDUdIASHqOIw.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVboggqs6Gy"
      },
      "source": [
        " - 시계열 데이터의 경우, 데이터 유형(주식 가격과 같은 `숫자형 순차데이터`, `텍스트`, `음성`, `비디오`)  에 따라서 불러오는 방식과 전처리 하는 방식이 다릅니다.\n",
        " \n",
        " - imdb 데이터셋을 불러와 입력데이터(x)와 결과데이터(y)가 어떻게 구성되어 있는지 살펴봅시다.\n",
        "\n",
        " - imdb 데이터셋 불러오기\n",
        "\n",
        "  - `num_words`: 단어의 빈도수가 높은 순서대로 숫자를 부여. `num_words=10000`이면 빈도수가 가장 높은 10000개의 단어만 불러오고, 나머지 단어들은 `oov (out-of-variable)` 형태로 불러옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWmGYaG_s6Gy"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so32RgUjy4t8",
        "outputId": "3bc71457-c68a-467f-a998-a00f3224a94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ4teAc6YYgo"
      },
      "source": [
        " - 텍스트 데이터에 대해서는 `숫자형`으로 인코딩을 해주셔야 하는데\n",
        "\n",
        "  - 1) 단어의 빈도수에 따라서 `사전(dictionary) **문자:숫자**`을 구성 \n",
        "\n",
        "  - 2) 정의해 놓은 사전을 이용해서 숫자형으로 인코딩. 단, 모든 단어들을 전부 불러올 수는 없으니 빈도수에 따라서 가장 빈도수가 높은 N개의 단어들만 숫자로 변환\n",
        "\n",
        "  - 3) 빈도수가 높은 N개에 포함되지 않는 단어들은 `OOV`로 표기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqzLD0us6G2"
      },
      "source": [
        "## Step 1-2) 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu1cG8M8CxDi"
      },
      "source": [
        " - RNN은 MLP와 다르게 임의의 길이의 시퀀스를 입력으로 받을 수 있으나 빠른 연산을 위해 **길이가 동일**한 시퀀스들을 입력으로 받습니다.\n",
        "  \n",
        " - 모든 입력 문장의 길이를 동일하게 하기 위해 순차데이터 전처리 함수들을 모아놓은 `tf.keras.preprocessing.sequence` 패키지의 `pad_sequences()` 함수가 사용됩니다.\n",
        "\n",
        "  - `pad_sequences()` 함수는 `문장의 길이를 얼마로 통일시킬지(maxlen)`를 입력으로 받아 \n",
        "  \n",
        "    - 지정된 길이보다 짧은 문장은 0을 추가해 길이를 맞추고 \n",
        "    \n",
        "    - 지정된 길이보다 긴 문장은 지정된 길이에 맞도록 자릅니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2gb5cy0s6G2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=64, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=64, padding=\"pre\", truncating=\"pre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_quSVC2s6G3"
      },
      "source": [
        " - 텍스트 전처리가 제대로 되었는지 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMoMxv3sD76C",
        "outputId": "b55fa7ae-fd77-4130-b97a-6fd42698b063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 64)\n",
            "(25000, 64)\n"
          ]
        }
      ],
      "source": [
        "print(x_train_RNN.shape)\n",
        "print(x_test_RNN.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF6j_bbss6G4"
      },
      "source": [
        "## Step 2) RNN 네트워크 구성\n",
        "\n",
        " - **layers** 모듈에 있는 **Input, SimpleRNN, Dense** 함수와 **Model** 함수를 이용하여 인공신경망을 설계해봅시다.\n",
        "\n",
        " - Input layer의 shape은 **sequence의 길이**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHJztkHpLpN2"
      },
      "source": [
        " - 각각의 단어를 one-hot encoding 해주었을 때 두 가지 문제가 발생합니다.\n",
        "\n",
        "  1. **희소 (sparse)** 입력으로 인한 **학습 부진** 문제\n",
        "\n",
        "      - 1 (one-hot)에 해당하는 가중치만 계산에 사용. 0에 해당하는 가중치들은 학습이 되지 않음.\n",
        "\n",
        "  2. **Out of Memory (OOM)**, 메모리 부족 문제\n",
        "      \n",
        " - 시계열 데이터 처리에서는 one-hot encoding (sparse encoding) 대신 dense embedding을 사용합니다 (**layers.Embedding()** 함수 이용)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yAFGpYV3wJR"
      },
      "outputs": [],
      "source": [
        "# seq_len: 시퀀스의 길이\n",
        "# dim_embedding: 각각의 단어들을 인코딩해줄 때 인코딩 시키고 싶은 차원 \n",
        "# dim_hidden: hidden state의 차원 (cell state의 차원은 hidden state의 차원과 동일)\n",
        "# model_name: RNN을 쓸 것이냐, LSTM을 쓸 것이냐\n",
        "\n",
        "def MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name):\n",
        "    # 입력 계층\n",
        "    review = layers.Input(shape=seq_len)\n",
        "\n",
        "    # 임베딩 계층\n",
        "    embedding = layers.Embedding(input_dim=num_words, output_dim=dim_embedding)(review)\n",
        "\n",
        "    # 시계열 데이터 특징 추출 계층\n",
        "    if model_name == \"RNN\":\n",
        "      hidden = layers.SimpleRNN(units=dim_hidden, activation=\"tanh\", return_sequences=False, return_state=False)(embedding)\n",
        "    elif model_name == \"LSTM\":\n",
        "      hidden = layers.LSTM(units=dim_hidden, activation=\"tanh\", return_sequences=False, return_state=False)(embedding)\n",
        "\n",
        "    # MLP를 이용한 최종 분류 계층\n",
        "    prob = layers.Dense(units=1, activation=\"sigmoid\")(hidden)\n",
        "\n",
        "    return Model(inputs=review, outputs=prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk3CC5f0s6G5"
      },
      "source": [
        "  - **return_sequences**\n",
        "\n",
        "    - `True`로 설정할 경우, 모든 cell의 hidden state들을 출력\n",
        "\n",
        "  - **return_state**\n",
        "\n",
        "    - `True`로 설정할 경우, 마지막 cell의 hidden state를 한 번 더 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWr_3a8Zs6G7"
      },
      "source": [
        "## Step 3) 손실함수 (Loss), 모델 업데이트 알고리즘 (Optimizer), 평가지표 (metrics) 설정 (**compile**)\n",
        "\n",
        "  - 분류 문제의 경우 손실함수(loss)로는 확률과 확률 사이의 차이를 측정하는데 특화된  `교차 엔트로피(Cross Entropy)`를 주로 사용합니다.\n",
        "   - 이진 분류 (2개의 클래스 분류; 양성/음성, 개/고양이): `binary_crossentropy`\n",
        "   - 다중 분류 (3개 이상의 클래스 분류; 개/고양이/사람):`categorical_crossentropy`\n",
        "  \n",
        "  - 모델 업데이트 알고리즘(optimizer)으로는 `경사하강법 (gradient descent)`에 기반한 `Adam`을 주로 사용합니다.\n",
        "   - `tf.keras.optimizers` 라이브러리 안에서 다양한 종류의 optimizer 사용 가능\n",
        "\n",
        "  - 평가지표(metrics)로는 `accuracy`를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQc2fa6tya2u"
      },
      "source": [
        "## Step 4) 설계한 인공신경망 학습 (**fit**)\n",
        "\n",
        "  - `batch_size`: parameter를 한 번 업데이트 할 때 몇 개의 데이터를 사용할 것인가\n",
        "\n",
        "  - `epochs`: 학습데이터를 총 몇 번 복습시킬 것인가\n",
        "\n",
        "  - `verbose`: 학습 경과를 보여줄 것인가\n",
        "\n",
        "  - `validation_data`: 입력했을 경우, 1 epoch이 끝날 때 마다 validation_data에 대한 모델의 성능 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i3EwDyW9EpV",
        "outputId": "33c1480f-bede-45ff-e33c-149006c65707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 64)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 64, 256)           2560000   \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 128)               49280     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,609,409\n",
            "Trainable params: 2,609,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 57s 69ms/step - loss: 0.5523 - accuracy: 0.7039 - val_loss: 0.4789 - val_accuracy: 0.7809\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 54s 70ms/step - loss: 0.4172 - accuracy: 0.8165 - val_loss: 0.5516 - val_accuracy: 0.7216\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 52s 67ms/step - loss: 0.3720 - accuracy: 0.8411 - val_loss: 0.4890 - val_accuracy: 0.7939\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.2990 - accuracy: 0.8779 - val_loss: 0.4876 - val_accuracy: 0.8009\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.2629 - accuracy: 0.8980 - val_loss: 0.5005 - val_accuracy: 0.7878\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f367f1d59d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "num_words = 10000\n",
        "seq_len = 64\n",
        "dim_embedding = 256\n",
        "dim_hidden = 128\n",
        "model_name = \"RNN\"\n",
        "\n",
        "# 모델 생성\n",
        "model = MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name)\n",
        "\n",
        "# 모델 출력\n",
        "model.summary()\n",
        "\n",
        "# 손실함수, 업데이트 알고리즘, 평가지표 설정\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 입력 데이터 전처리\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "# 학습\n",
        "model.fit(x_train_RNN, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_RNN, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCo4yzGZt0LA",
        "outputId": "b3d4047b-f690-40ed-d926-6853107a41ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 64)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 64, 256)           2560000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,757,249\n",
            "Trainable params: 2,757,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.4382 - accuracy: 0.7916 - val_loss: 0.3907 - val_accuracy: 0.8307\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.2818 - accuracy: 0.8841 - val_loss: 0.3797 - val_accuracy: 0.8313\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.1908 - accuracy: 0.9248 - val_loss: 0.4487 - val_accuracy: 0.8220\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.1260 - accuracy: 0.9539 - val_loss: 0.5945 - val_accuracy: 0.8085\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.0795 - accuracy: 0.9718 - val_loss: 0.6531 - val_accuracy: 0.8084\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x26e13e926c8>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_words = 10000\n",
        "seq_len = 64\n",
        "dim_embedding = 256\n",
        "dim_hidden = 128\n",
        "model_name = \"LSTM\"\n",
        "\n",
        " 모델 생성\n",
        "model = MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name)\n",
        "\n",
        "# 모델 출력\n",
        "model.summary()\n",
        "\n",
        "# 손실함수, 업데이트 알고리즘, 평가지표 설정\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 입력 데이터 전처리\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "# 학습\n",
        "model.fit(x_train_RNN, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_RNN, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tMVOvisoA7D",
        "outputId": "92b0c2fd-5832-4447-b068-8b577a064f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 512)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 512, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 128)               49280     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,609,409\n",
            "Trainable params: 2,609,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 196s 251ms/step - loss: 0.5730 - accuracy: 0.6886 - val_loss: 0.4773 - val_accuracy: 0.7922\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 197s 252ms/step - loss: 0.4556 - accuracy: 0.7874 - val_loss: 0.4874 - val_accuracy: 0.7717\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 197s 252ms/step - loss: 0.3325 - accuracy: 0.8611 - val_loss: 0.4329 - val_accuracy: 0.8022\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 197s 252ms/step - loss: 0.3549 - accuracy: 0.8456 - val_loss: 0.4617 - val_accuracy: 0.8031\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 196s 250ms/step - loss: 0.4217 - accuracy: 0.8050 - val_loss: 0.5941 - val_accuracy: 0.7191\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x26e5b78f248>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_words = 10000\n",
        "seq_len = 512\n",
        "dim_embedding = 256\n",
        "dim_hidden = 128\n",
        "model_name = \"RNN\"\n",
        "\n",
        "# 모델 생성\n",
        "model = MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name)\n",
        "\n",
        "# 모델 출력\n",
        "model.summary()\n",
        "\n",
        "# 손실함수, 업데이트 알고리즘, 평가지표 설정\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 입력 데이터 전처리\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "# 학습\n",
        "model.fit(x_train_RNN, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_RNN, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EannwVUL7new",
        "outputId": "510330d2-cd55-4279-eae6-89427d4ea8ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 512)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 512, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,757,249\n",
            "Trainable params: 2,757,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 42s 53ms/step - loss: 0.4570 - accuracy: 0.7802 - val_loss: 0.3740 - val_accuracy: 0.8388\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 42s 53ms/step - loss: 0.2566 - accuracy: 0.8984 - val_loss: 0.3552 - val_accuracy: 0.8667\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.1963 - accuracy: 0.9259 - val_loss: 0.4269 - val_accuracy: 0.8573\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 42s 53ms/step - loss: 0.1465 - accuracy: 0.9454 - val_loss: 0.3796 - val_accuracy: 0.8650\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.1277 - accuracy: 0.9548 - val_loss: 0.4612 - val_accuracy: 0.8616\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x26e0b543cc8>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_words = 10000\n",
        "seq_len = 512\n",
        "dim_embedding = 256\n",
        "dim_hidden = 128\n",
        "model_name = \"LSTM\"\n",
        "\n",
        "# 모델 생성\n",
        "model = MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name)\n",
        "\n",
        "# 모델 출력\n",
        "model.summary()\n",
        "\n",
        "# 손실함수, 업데이트 알고리즘, 평가지표 설정\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 입력 데이터 전처리\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "# 학습\n",
        "model.fit(x_train_RNN, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_RNN, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m6OVbPr6rDG",
        "outputId": "ede1d0f2-5ddb-4204-a13d-59818136eef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 1024, 256)         2560000   \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 128)               49280     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,609,409\n",
            "Trainable params: 2,609,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 427s 546ms/step - loss: 0.6543 - accuracy: 0.5954 - val_loss: 0.6191 - val_accuracy: 0.6354\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 421s 538ms/step - loss: 0.5338 - accuracy: 0.7253 - val_loss: 0.6240 - val_accuracy: 0.6318\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 424s 543ms/step - loss: 0.4629 - accuracy: 0.7770 - val_loss: 0.4644 - val_accuracy: 0.7962\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 425s 544ms/step - loss: 0.5095 - accuracy: 0.7292 - val_loss: 0.6337 - val_accuracy: 0.6566\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 422s 540ms/step - loss: 0.4329 - accuracy: 0.7907 - val_loss: 0.5563 - val_accuracy: 0.7396\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x26e5d4253c8>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_words = 10000\n",
        "seq_len = 1024\n",
        "dim_embedding = 256\n",
        "dim_hidden = 128\n",
        "model_name = \"RNN\"\n",
        "\n",
        "# 모델 생성\n",
        "model = MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name)\n",
        "\n",
        "# 모델 출력\n",
        "model.summary()\n",
        "\n",
        "# 손실함수, 업데이트 알고리즘, 평가지표 설정\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 입력 데이터 전처리\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "# 학습\n",
        "model.fit(x_train_RNN, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_RNN, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoYZA6MH3wJU",
        "outputId": "af883561-7e8d-41ac-a938-875958812fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 1024, 256)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,757,249\n",
            "Trainable params: 2,757,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.4621 - accuracy: 0.7781 - val_loss: 0.3309 - val_accuracy: 0.8652\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 69s 88ms/step - loss: 0.2708 - accuracy: 0.8916 - val_loss: 0.3171 - val_accuracy: 0.8740\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 69s 88ms/step - loss: 0.1999 - accuracy: 0.9254 - val_loss: 0.3335 - val_accuracy: 0.8715\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 69s 88ms/step - loss: 0.1385 - accuracy: 0.9476 - val_loss: 0.3773 - val_accuracy: 0.8709\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 69s 88ms/step - loss: 0.1006 - accuracy: 0.9652 - val_loss: 0.4266 - val_accuracy: 0.8645\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x26e5f0aa9c8>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_words = 10000\n",
        "seq_len = 1024\n",
        "dim_embedding = 256\n",
        "dim_hidden = 128\n",
        "model_name = \"LSTM\"\n",
        "\n",
        "# 모델 생성\n",
        "model = MyModel(num_words, seq_len, dim_embedding, dim_hidden, model_name)\n",
        "\n",
        "# 모델 출력\n",
        "model.summary()\n",
        "\n",
        "# 손실함수, 업데이트 알고리즘, 평가지표 설정\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 입력 데이터 전처리\n",
        "x_train_RNN = pad_sequences(sequences=x_train, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "x_test_RNN = pad_sequences(sequences=x_test, maxlen=seq_len, padding=\"pre\", truncating=\"pre\")\n",
        "\n",
        "# 학습\n",
        "model.fit(x_train_RNN, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_RNN, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}