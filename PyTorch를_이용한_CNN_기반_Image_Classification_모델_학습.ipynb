{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57k3sN6UqHa"
      },
      "source": [
        "# Title: PyTorchë¥¼ ì´ìš©í•œ í•©ì„±ê³± ì‹ ê²½ë§ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt17Uz2tCAwB"
      },
      "source": [
        " - `í•©ì„±ê³±(convolution)` ì—°ì‚°\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_6.webp\" width=500>\n",
        " \n",
        "<img src='https://datadiving.dothome.co.kr/Deep%203_7.jpg' width=700>\n",
        "\n",
        " - `Convolution`ì˜ ê¸°ëŠ¥\n",
        "  - `Convolution`ì€ `filter (kernel)`ì„ í†µí•˜ì—¬ íŠ¹ì • **íŠ¹ì§•(feature)ì˜ ìœ ë¬´ ë° ìœ„ì¹˜**ë¥¼ ì‹ë³„í•˜ëŠ”ë° íŠ¹í™”ëœ ì—°ì‚°ì…ë‹ˆë‹¤.\n",
        "  - ì´ëŸ¬í•œ ì´ìœ ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (image classification) ì™¸ì—ë„ `ê°ì²´ ìœ„ì¹˜ ì‹ë³„ (object localization)`, `ê°ì²´ íƒì§€ (object detection)`, `ì´ë¯¸ì§€ ë¶„í•  (image segmentation)`ê³¼ ê°™ì€ ë§ì€ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ CNNì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        " \n",
        " <img src=\"https://datadiving.dothome.co.kr/Deep%203_1.jpg\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CNNì— ê¸°ë°˜í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ëŠ” \n",
        "  - 1) ì´ë¯¸ì§€ì˜ **íŠ¹ì§•ì„ ì¶”ì¶œ**í•˜ëŠ” **CNN** íŒŒíŠ¸ì™€ (`Conv2D`, `MaxPooling2D` ì´ìš©)\n",
        "    - **ì¶”ì¶œí•˜ëŠ” íŠ¹ì§•ì„ ì ì  êµ¬ì²´í™”(ì˜ˆ: í…Œë‘ë¦¬ -> ëˆˆ,ì½”,ê·€ -> ì–¼êµ´)**í•˜ê¸° ìœ„í•˜ì—¬ **Conv layer ì¤‘ê°„ ì¤‘ê°„ì— Pooling layer ì¶”ê°€**\n",
        "\n",
        "        <img src = \"https://datadiving.dothome.co.kr/Deep%203_9.jpeg\" width=1000>\n",
        "\n",
        "    - ì¼ë°˜ì ìœ¼ë¡œ íŠ¹ì§•ì´ ì ì  êµ¬ì²´í™” ë ìˆ˜ë¡ ë§ì€ ìˆ˜ì˜ íŠ¹ì§• ì¶”ì¶œ, **filterì˜ ê°¯ìˆ˜ ì¦ê°€**\n",
        "    - ë§ì€ ìˆ˜ì˜ Conv layerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ **zero padding** ì‚¬ìš©\n",
        "      - **Conv layerë¥¼ í†µê³¼í–ˆì„ ë•Œ ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ì¤„ì–´ë“œëŠ” ê²ƒ ë°©ì§€**\n",
        "\n",
        "  - 2) **ì¶”ì¶œëœ íŠ¹ì§•ì„ ì´ìš©í•˜ì—¬ classificationì„ ìˆ˜í–‰**í•˜ëŠ” **MLP** íŒŒíŠ¸ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n",
        "    - CNNì˜ ì¶œë ¥ì€ 3ì°¨ì›ì´ê¸° ë•Œë¬¸ì— MLPì— ë„£ì–´ì¤„ ë•Œ **1ì°¨ì›ìœ¼ë¡œ reshape** (`Flatten` ì´ìš©) í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_4.png\" width=1100> "
      ],
      "metadata": {
        "id": "A01Xt2tQfuZT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGU_-H2BC1a"
      },
      "source": [
        " - MLPì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ `conv layer`ì˜ ìˆ˜ì™€ `filter (feature map)`ì˜ ìˆ˜ëŠ” **ì‹¤í—˜ì **ìœ¼ë¡œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ìƒì„± ë° í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì…”ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        " - ë‹¤ìŒ ì„¸ ê°€ì§€ ê°’ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê·¸ì— í•´ë‹¹í•˜ëŠ” í•©ì„±ê³± ì‹ ê²½ë§ì„ ìƒì„±í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤:\n",
        "   - ì…ë ¥ ë°ì´í„° ëª¨ì–‘ `input_shape`\n",
        "   - ê²°ê³¼ ë°ì´í„° ì°¨ì› `output_dim`\n",
        "   - `conv layer`ì˜ `filter` ìˆ˜ë¥¼ ëª¨ì•„ë†“ì€ `num_filters_list` (ì˜ˆ: num_filters_list = [16, 'max_pool', 32, 'max_pool', 64, 'max_pool', 128, 'max_pool'])\n",
        "    - 'max_pool'ì¸ ê²½ìš° `pooling`ì„ ì ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhmgNBWgi3g"
      },
      "source": [
        "### Step 0) PyTorch íŒ¨í‚¤ì§€ import"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ì°¸ê³ : **Tensorflow** íŒ¨í‚¤ì§€\n",
        "\n",
        "  - `tensorflow.keras.layers (layers)`: ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„¤ê³„í•  ë•Œ ì¸µ(layer) ê´€ë ¨ í•¨ìˆ˜ë“¤(ì˜ˆ:`Dense`, `Conv2D`, `MaxPooling2D`, `SimpleRNN`, `LSTM`)ì„ ëª¨ì•„ë†“ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        "  - `tensorflow.keras.models.Model (Model)`: ìƒì„±í•œ ì¸µë“¤ì„ ì—°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ êµ¬ì„±í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "  - `tensorflow.keras.datasets`: TensorFlowì—ì„œ ë”¥ëŸ¬ë‹ ì‹¤ìŠµì„ ìœ„í•´ ì œê³µí•´ì£¼ëŠ” ë°ì´í„°ì…‹ (ì˜ˆ: `mnist`, `cifar10`, `cifar100`, `imdb`)"
      ],
      "metadata": {
        "id": "hGfyQNsd6FAB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn6sZJDXgi3i"
      },
      "source": [
        "import torch\n",
        "\n",
        "## ë°ì´í„° ì „ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "## ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± ê´€ë ¨ íŒ¨í‚¤ì§€\n",
        "import torch.nn as nn\n",
        "\n",
        "## ë”¥ëŸ¬ë‹ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜ ê´€ë ¨ íŒ¨í‚¤ì§€\n",
        "import torch.optim as optim\n",
        "\n",
        "## ë‹¤ì°¨ì› ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import numpy as np\n",
        "\n",
        "## ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) ë°ì´í„° ì „ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€\n",
        "\n",
        " - `torchvision.datasets`: ë‹¤ì–‘í•œ datasetë“¤ì„ í¬í•¨í•˜ê³  ìˆëŠ” íŒ¨í‚¤ì§€ (https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        " - `torchvision.transforms`: ë°ì´í„° ë³€í˜•ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        " - `torch.utils.data.DataLoader`: ë°ì´í„°ë¥¼ `batch size` í¬ê¸°ë¡œ ë¶„í• í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        "\n",
        "1) ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± ê´€ë ¨ íŒ¨í‚¤ì§€\n",
        "\n",
        "  - `torch.nn (nn)`: ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìƒì„±í•  ë•Œ ì¸µ(layer) ê´€ë ¨ í•¨ìˆ˜ë“¤(ì˜ˆ: Linear, Conv2d, MaxPool2d, ReLU, BatchNorm2d)ì„ ëª¨ì•„ë†“ì€ íŒ¨í‚¤ì§€\n",
        "\n",
        "  - `torch.nn.Module (Module)`: ìƒì„±í•œ ì¸µë“¤ì„ ì—°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ êµ¬ì„±í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤\n",
        "\n",
        "2) ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ê´€ë ¨ íŒ¨í‚¤ì§€\n",
        "\n",
        "  - `torch.optim (optim)`: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜(`optimizer`)ë“¤ì„ ëª¨ì•„ë†“ì€ íŒ¨í‚¤ì§€ "
      ],
      "metadata": {
        "id": "GUbz4izECgN1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn87b6T7c7Zj"
      },
      "source": [
        " ### Step 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIliloq9MD9H"
      },
      "source": [
        " - CIFAR10 ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yRGukIoc7Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84a6423-bf0f-4715-c1bb-e9d104b6467d"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), \n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "     )\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"../data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"../data\", train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZwAXBMtnPL7"
      },
      "source": [
        " - **Data Augmentation (ë°ì´í„° ì¦ê°•)**\n",
        "  \n",
        "  - ì›ë³¸ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê¸°ë²• (**ë°ì´í„° ë‹¤ì–‘í™”**)\n",
        "      - ë³€í˜•ì˜ ì˜ˆ) íšŒì „(rotation), ë°˜ì „ or ëŒ€ì¹­(flip), ì˜ë¼ë‚´ê¸°(crop)\n",
        "\n",
        "      <img src=\"https://miro.medium.com/max/1400/0*LR1ZQucYW96prDte\" width=700>\n",
        "\n",
        "      - ì°¸ê³ ) `ResNet` í•™ìŠµì—ë„ `Data Augmentation` ê¸°ë²•ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤! (https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "  - `Data Augmenetation`ì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ?\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_data%20augmentation.jpg' border='0'></a>\n",
        "\n",
        "    **Flip**\n",
        "\n",
        "    - ì™¼ìª½ë§Œ ë°”ë¼ë³´ëŠ” ê³ ì–‘ì´ê°€ train ë•Œ ì£¼ì–´ì¡Œë‹¤ë©´ ì˜¤ë¥¸ìª½ì„ ë³´ëŠ” ê³ ì–‘ì´ëŠ” test ë•Œ ë§ì¶”ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    \n",
        "    - ì´ ê²½ìš°, ì™¼ìª½ë§Œ ë°”ë¼ë³´ëŠ” ê³ ì–‘ì´ë¥¼ ì¢Œìš° ëŒ€ì¹­ì‹œì¼œ ëª¨ë‘ í•™ìŠµì— ì‚¬ìš©í•˜ë©´ test ë•Œ ê³ ì–‘ì´ê°€ ì–´ëŠ ìª½ì„ ë³´ë”ë¼ë„ ë§ì¶œ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_data%20augmentation%202.jpg' border='0'></a>\n",
        "\n",
        "    **Crop**\n",
        "\n",
        "    - ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ê¼¬ë¦¬ì™€ ê·€, ìˆ˜ì—¼, ëˆˆì´ë¼ëŠ” íŠ¹ì§•ìœ¼ë¡œ ê³ ì–‘ì´ì¸ì§€ë¥¼ íŒë‹¨í•œë‹¤ë©´ ê³ ì–‘ì´ê°€ ì¹¨ëŒ€ ë°‘ì— ìˆ¨ì–´ìˆì–´ ê¼¬ë¦¬ë§Œ ë³´ì´ëŠ” ê²½ìš° ê³ ì–‘ì´ë¼ê³  íŒë‹¨í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "    - ì´ ê²½ìš°, ê³ ì–‘ì´ì˜ ê° ë¶€ë¶„ì„ ì˜ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ê¼¬ë¦¬ ë¶€ë¶„ë§Œ ì˜ë¼ì„œ ë„£ì–´ì¤€ë‹¤ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_%EB%B0%9D%EA%B8%B0%20%EC%A1%B0%EC%A0%88.jpg' border='0'></a>\n",
        "\n",
        "    **ğŸ’¡ ë°ê¸° ì¡°ì ˆ**  \n",
        "    - ë§Œì•½ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ì•±ì„ ë§Œë“¤ê²Œ ë˜ë©´, ì‚¬ì§„ì„ ì°ëŠ” ì‚¬ëŒë§ˆë‹¤ ë¹›ì˜ ì–‘ì´ ë‹¤ë¥´ê² ì£ ? ë”¥ëŸ¬ë‹ì´ ë™ì¼í•˜ê²Œ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì–´ë‘ìš´ ê²ƒë¶€í„° ë°ì€ ê²ƒê¹Œì§€ ëª¨ë“  ì‚¬ì§„ì˜ ë°ê¸°ë¥¼ ì¡°ì ˆí•´ì„œ ë„£ì–´ì¤ì‹œë‹¤!    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PyTorchì—ì„œëŠ” `torchvision.transforms` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•˜ì—¬ `data augmentation`ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
      ],
      "metadata": {
        "id": "bix_MqiglFJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        transforms.RandomCrop(size=32, padding=4),\n",
        "        transforms.RandomHorizontalFlip()\n",
        "    ]\n",
        ")\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)"
      ],
      "metadata": {
        "id": "0DYwoSymlgJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a14b67-dcd2-4077-fadb-be58f157d92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ë¿ë§Œ ì•„ë‹ˆë¼, `DataLoader` í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ì„ `ë°°ì¹˜ (batch)` ë‹¨ìœ„ë¡œ ìª¼ê°¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "WA0mtqadl3Mz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqMz8cFpPkaV"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x45v3oflpl1d"
      },
      "source": [
        "### Step 2) ì¸ê³µì‹ ê²½ë§ ìƒì„±\n",
        "\n",
        " - PyTorchì—ì„œëŠ” `torch.nn` ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‚´ì˜ í•¨ìˆ˜ë“¤ê³¼ `í´ë˜ìŠ¤ (class)` ìš©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¸ê³µì‹ ê²½ë§ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "    - **nn.Conv2d**: í•©ì„±ê³± ê³„ì¸µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    - **nn.MaxPool2d**: í’€ë§ ê³„ì¸µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    - **nn.Flatten**: ë‹¤ì°¨ì› ë°ì´í„°ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™” ì‹œì¼œì£¼ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    - **nn.Linear**: ì „ê²°í•©ê³„ì¸µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyOPgcNBPvqI"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=\"same\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=\"same\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"same\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=2048, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = nn.ReLU()(self.conv1(x))\n",
        "        pool1 = self.pool1(conv1)\n",
        "\n",
        "        conv2 = nn.ReLU()(self.conv2(pool1))\n",
        "        pool2 = self.pool2(conv2)\n",
        "\n",
        "        conv3 = nn.ReLU()(self.conv3(pool2))\n",
        "        pool3 = self.pool3(conv3)\n",
        "\n",
        "        flatten = self.flatten(pool3)\n",
        "\n",
        "        logits = self.fc1(flatten)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWTqGX4XNdY0"
      },
      "source": [
        " - ì˜ë„ì— ë§ê²Œ ë„¤íŠ¸ì›Œí¬ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤!\n",
        "\n",
        "    - **summary** í•¨ìˆ˜ ì´ìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGpJJLNyDFJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ed205a-fc5f-4091-e67f-967fd8be5cbe"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = MyModel()\n",
        "model.to(\"cuda\")\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "         MaxPool2d-2           [-1, 32, 16, 16]               0\n",
            "            Conv2d-3           [-1, 64, 16, 16]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5            [-1, 128, 8, 8]          73,856\n",
            "         MaxPool2d-6            [-1, 128, 4, 4]               0\n",
            "           Flatten-7                 [-1, 2048]               0\n",
            "            Linear-8                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.56\n",
            "Params size (MB): 0.43\n",
            "Estimated Total Size (MB): 1.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-tYaD0quUI7"
      },
      "source": [
        "### Step 3) í•™ìŠµ (Training)\n",
        " \n",
        " - ì†ì‹¤ í•¨ìˆ˜ (Loss) ë° ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜ (Optimizer) ì„¤ì •\n",
        "    \n",
        "    - ì†ì‹¤í•¨ìˆ˜ë¡œëŠ” **êµì°¨ì—”íŠ¸ë¡œí”¼ì˜¤ì°¨ (cross entropy)**ë¥¼\n",
        "    \n",
        "    - ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” **adam** optimizerë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "        - `torch.optim (optim)`: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜(`optimizer`)ë“¤ì„ ëª¨ì•„ë†“ì€ íŒ¨í‚¤ì§€ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btwwUUdFuaVS"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PyTorchì—ì„œëŠ” GPUë¥¼ ì‚¬ìš©í•  ê²½ìš°, **ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ëª¨ë‘ GPUì— ì˜¬ë ¤ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.**"
      ],
      "metadata": {
        "id": "nw_ICSpNoyBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8xDcs8CvovXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca106dbc-5d90-42e5-e8d5-f07d2944970d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmfGFWpNulDu"
      },
      "source": [
        " - ğŸ‘¨ğŸ»â€ğŸ« í•™ìŠµ ìˆœì„œ\n",
        "\n",
        "    1. ì…ë ¥ (x)ì— ëŒ€í•œ ê²°ê³¼ ì˜ˆì¸¡ -> y_pred\n",
        "\n",
        "    2. y_predì™€ ì •ë‹µ (y_true)ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ ê³„ì‚° -> **criterion(y_pred, y_true)**\n",
        "\n",
        "    3. ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ -> **optimizer.step()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSqvJyw9uhW5",
        "outputId": "7c75916e-89e3-4a29-e3d4-8194dc0ddda1"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    running_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # ì…ë ¥ì— ëŒ€í•œ ê²°ê³¼ ì˜ˆì¸¡\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        # ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì„ ë¹„êµí•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ ê³„ì‚°\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "\n",
        "        # ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            images, labels = batch\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}, Train Loss: {running_loss / total}, Train Accuracy: {100 * correct / total}')\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 0.04507628124833107, Train Accuracy: 57.25\n",
            "Test Accuracy: 60.36\n",
            "Epoch: 2, Train Loss: 0.03320949960827827, Train Accuracy: 66.852\n",
            "Test Accuracy: 69.42\n",
            "Epoch: 3, Train Loss: 0.028798986775279044, Train Accuracy: 69.17\n",
            "Test Accuracy: 71.48\n",
            "Epoch: 4, Train Loss: 0.02636519924044609, Train Accuracy: 72.058\n",
            "Test Accuracy: 73.15\n",
            "Epoch: 5, Train Loss: 0.0246919292140007, Train Accuracy: 74.768\n",
            "Test Accuracy: 75.06\n",
            "Epoch: 6, Train Loss: 0.023364896874427797, Train Accuracy: 75.782\n",
            "Test Accuracy: 75.76\n",
            "Epoch: 7, Train Loss: 0.022320393640995025, Train Accuracy: 76.148\n",
            "Test Accuracy: 77.32\n",
            "Epoch: 8, Train Loss: 0.02160057497948408, Train Accuracy: 77.264\n",
            "Test Accuracy: 77.53\n",
            "Epoch: 9, Train Loss: 0.02088353258550167, Train Accuracy: 78.264\n",
            "Test Accuracy: 78.16\n",
            "Epoch: 10, Train Loss: 0.02063079702347517, Train Accuracy: 77.912\n",
            "Test Accuracy: 77.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2) **ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)**"
      ],
      "metadata": {
        "id": "f4yu17XK27CM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `Gradient Vanishing Problem`: `Backpropagation`ì— ì˜í•´ gradientê°€ ì•ë‹¨ìœ¼ë¡œ ì „íŒŒë˜ë©´ì„œ ì ì  ì˜…ì–´ì§€ê²Œ ë˜ì–´ ë„ˆë¬´ ì‘ì•„ì ¸ì„œ **ì†Œë©¸**í•˜ê²Œ ë˜ëŠ” ë¬¸ì œ\n",
        "\n",
        "    <img src=\"https://t1.daumcdn.net/cfile/tistory/997E1B4C5BB6EAF239\" width=700>\n",
        "\n",
        "    - Layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ `í™œì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’(gradient)`ì´ ê³„ì† ê³±í•´ì§€ëŠ”ë°, ê¸°ìš¸ê¸°ê°€ ì™„ë§Œí•œ ì˜ì—­ì˜ ì…ë ¥ê°’ì´ ì£¼ì–´ì§€ëŠ” ê²½ìš° ì•ë‹¨ì˜ `gradient`ê°€ ì•„ì£¼ ì‘ì•„ ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ"
      ],
      "metadata": {
        "id": "536TWaamvMjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `Internal Covariate Shift`: Layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë³€í™”í•˜ëŠ” í˜„ìƒ\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_internal%20Covariate%20Shift%202.jpg' width=700>\n",
        "\n",
        "    - Layerê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì´ì „ì— ë” ë§ì€ layerë¥¼ í†µê³¼í•˜ê¸° ë•Œë¬¸ì— ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë§ì´ ë³€í•˜ê²Œ ëœë‹¤.\n",
        "\n",
        "    <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/3.png\" width=700>\n",
        "\n",
        "    - ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ì ì  ë³€í˜•ë˜ì–´ `gradient`ê°€ ì‘ì€ ì˜ì—­ì— ë„ë‹¬í•˜ê²Œ ëœë‹¤ë©´ `vanishing gradient`ë¬¸ì œê°€ ë°œìƒí•œë‹¤."
      ],
      "metadata": {
        "id": "6TTMpW6ctz3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/4.png\" width=700>\n",
        "\n",
        "\n",
        " - `ë°°ì¹˜ ì •ê·œí™” (Batch Normalization)`: ê° ë ˆì´ì–´ë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ **ì •ê·œí™” (normalization)**í•˜ëŠ” ë ˆì´ì–´ë¥¼ ë‘ì–´ ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë³€í˜•ë˜ì§€ ì•Šë„ë¡ ì¡°ì ˆí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜\n",
        "\n",
        "  - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹¤ì‹œí•˜ê¸° ë•Œë¬¸ì— `batch normalization`ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.\n",
        "\n",
        "  - í™œì„±í•¨ìˆ˜ì˜ ì…ë ¥ê°’ì˜ ë¶„í¬ë¥¼ ì¡°ì ˆí•´ì£¼ëŠ” ê²ƒì´ ëª©ì ì´ë¯€ë¡œ **í™œì„±í•¨ìˆ˜** ì•ì— ë„£ì–´ì¤€ë‹¤.\n",
        "\n",
        "  - ì•Œê³ ë¦¬ì¦˜ í…Œì´ë¸”\n",
        "\n",
        "    <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcFYkLE%2FbtqEcUnlXKy%2FZbGZNjObjo2gL2xss8zYzk%2Fimg.png\" width=500>\n",
        "\n",
        "  - $\\gamma, \\beta$ë¡œ `scale and shift`ë¥¼ í•´ì£¼ëŠ” ì´ìœ ?\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/1400/1*0yhJ7DbhOX-tRUseljjYoA.png\" width=700>"
      ],
      "metadata": {
        "id": "HV58Sr6HuKnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **VGG16**ì— `BatchNormalization`ì„ ì¶”ê°€í•œ ëª¨ë¸ì„ PyTorchë¥¼ ì´ìš©í•˜ì—¬ ìƒì„±í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/857/1*AqqArOvacibWqeulyP_-8Q.png\" width=700>"
      ],
      "metadata": {
        "id": "qq7hIl2t_fAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MyModel(input_shape, output_dim, num_filters_list, use_batch_norm):\n",
        "  # ì…ë ¥ê³„ì¸µ (Input Layer)\n",
        "  img = layers.Input(shape=input_shape) # cifar10ì˜ ê²½ìš°ëŠ” input_shape=[32, 32, 3]\n",
        "\n",
        "  # íŠ¹ì§• ì¶”ì¶œ íŒŒíŠ¸ - CNN\n",
        "  h = img\n",
        "  for num_filters in num_filters_list:\n",
        "    if num_filters == \"max_pool\": \n",
        "      h = layers.MaxPooling2D(pool_size=(2, 2))(h)\n",
        "    else:\n",
        "      h = layers.Conv2D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\")(h) # convolution \n",
        "      if use_batch_norm == True:\n",
        "        h = layers.BatchNormalization()(h) # batch normalization\n",
        "      h = layers.ReLU()(h) # activation\n",
        "\n",
        "  # ë¶„ë¥˜ íŒŒíŠ¸ - MLP\n",
        "  mlp_input = layers.Flatten()(h)\n",
        "  prob = layers.Dense(units=output_dim, activation=\"softmax\")(mlp_input)\n",
        "\n",
        "  # ì „ì²´ ëª¨ë¸\n",
        "  return Model(inputs=img, outputs=prob)"
      ],
      "metadata": {
        "id": "Ea2_0wHNlQCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SQEdnDltFTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3) **ResNet** êµ¬í˜„\n",
        "\n",
        " - ResNetì—ì„œì˜ key ideaëŠ” 2ê°œì˜ layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ `skip connection`ì„ ë„£ì–´ì£¼ì–´`backpropagation` ê³¼ì •ì—ì„œ **gradientê°€ ì˜ íë¥´ë„ë¡ í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.\n",
        " \n",
        " <img src=\"https://images.velog.io/images/junyoung9696/post/3137e50c-b52f-4cdd-8ae8-2faf497efe84/r10.png\" width=500>\n",
        "\n",
        " - ResNet ë…¼ë¬¸(https://arxiv.org/pdf/1512.03385.pdf)ì„ ì°¸ê³ í•˜ì—¬ CIFAR10 ì´ë¯¸ì§€ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "  <img src=\"https://blog.kakaocdn.net/dn/bQfaUX/btqYAtD1KcX/Zdc4DLFzR9SoJYBlO6M1uK/img.png\" width=700>"
      ],
      "metadata": {
        "id": "UfXBunC6Ow_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Step 1) Residual Block êµ¬ì„±"
      ],
      "metadata": {
        "id": "mHLLZU95xCSt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_KSlHjXwo14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}