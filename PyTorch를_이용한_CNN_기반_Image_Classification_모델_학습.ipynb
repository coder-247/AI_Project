{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57k3sN6UqHa"
      },
      "source": [
        "# Title: PyTorch를 이용한 합성곱 신경망 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt17Uz2tCAwB"
      },
      "source": [
        " - `합성곱(convolution)` 연산\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_6.webp\" width=500>\n",
        " \n",
        "<img src='https://datadiving.dothome.co.kr/Deep%203_7.jpg' width=700>\n",
        "\n",
        " - `Convolution`의 기능\n",
        "  - `Convolution`은 `filter (kernel)`을 통하여 특정 **특징(feature)의 유무 및 위치**를 식별하는데 특화된 연산입니다.\n",
        "  - 이러한 이유로 이미지 분류 (image classification) 외에도 `객체 위치 식별 (object localization)`, `객체 탐지 (object detection)`, `이미지 분할 (image segmentation)`과 같은 많은 어플리케이션에서 CNN이 사용되었습니다.\n",
        " \n",
        " <img src=\"https://datadiving.dothome.co.kr/Deep%203_1.jpg\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CNN에 기반한 이미지 분류 네트워크는 \n",
        "  - 1) 이미지의 **특징을 추출**하는 **CNN** 파트와 (`Conv2D`, `MaxPooling2D` 이용)\n",
        "    - **추출하는 특징을 점점 구체화(예: 테두리 -> 눈,코,귀 -> 얼굴)**하기 위하여 **Conv layer 중간 중간에 Pooling layer 추가**\n",
        "\n",
        "        <img src = \"https://datadiving.dothome.co.kr/Deep%203_9.jpeg\" width=1000>\n",
        "\n",
        "    - 일반적으로 특징이 점점 구체화 될수록 많은 수의 특징 추출, **filter의 갯수 증가**\n",
        "    - 많은 수의 Conv layer를 사용하기 위하여 **zero padding** 사용\n",
        "      - **Conv layer를 통과했을 때 이미지의 크기가 줄어드는 것 방지**\n",
        "\n",
        "  - 2) **추출된 특징을 이용하여 classification을 수행**하는 **MLP** 파트로 나뉩니다.\n",
        "    - CNN의 출력은 3차원이기 때문에 MLP에 넣어줄 때 **1차원으로 reshape** (`Flatten` 이용) 해주어야 합니다.\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_4.png\" width=1100> "
      ],
      "metadata": {
        "id": "A01Xt2tQfuZT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGU_-H2BC1a"
      },
      "source": [
        " - MLP에서와 마찬가지로 `conv layer`의 수와 `filter (feature map)`의 수는 **실험적**으로 결정되기 때문에 여러 개의 모델을 생성 및 학습하여 성능을 비교해보셔야 합니다.\n",
        "\n",
        " - 다음 세 가지 값을 입력으로 받아 그에 해당하는 합성곱 신경망을 생성해주는 함수를 만들어봅시다:\n",
        "   - 입력 데이터 모양 `input_shape`\n",
        "   - 결과 데이터 차원 `output_dim`\n",
        "   - `conv layer`의 `filter` 수를 모아놓은 `num_filters_list` (예: num_filters_list = [16, 'max_pool', 32, 'max_pool', 64, 'max_pool', 128, 'max_pool'])\n",
        "    - 'max_pool'인 경우 `pooling`을 적용하는 것으로 생각해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhmgNBWgi3g"
      },
      "source": [
        "### Step 0) PyTorch 패키지 import"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 참고: **Tensorflow** 패키지\n",
        "\n",
        "  - `tensorflow.keras.layers (layers)`: 딥러닝 네트워크를 설계할 때 층(layer) 관련 함수들(예:`Dense`, `Conv2D`, `MaxPooling2D`, `SimpleRNN`, `LSTM`)을 모아놓은 라이브러리\n",
        "\n",
        "  - `tensorflow.keras.models.Model (Model)`: 생성한 층들을 연합하여 하나의 모델로 구성할 때 사용하는 함수\n",
        "\n",
        "  - `tensorflow.keras.datasets`: TensorFlow에서 딥러닝 실습을 위해 제공해주는 데이터셋 (예: `mnist`, `cifar10`, `cifar100`, `imdb`)"
      ],
      "metadata": {
        "id": "hGfyQNsd6FAB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn6sZJDXgi3i"
      },
      "source": [
        "import torch\n",
        "\n",
        "## 데이터 전처리 관련 패키지\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "## 딥러닝 모델 생성 관련 패키지\n",
        "import torch.nn as nn\n",
        "\n",
        "## 딥러닝 모델 업데이트 알고리즘 관련 패키지\n",
        "import torch.optim as optim\n",
        "\n",
        "## 다차원 데이터 처리를 위한 라이브러리\n",
        "import numpy as np\n",
        "\n",
        "## 데이터 시각화를 위한 라이브러리\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) 데이터 전처리 관련 패키지\n",
        "\n",
        " - `torchvision.datasets`: 다양한 dataset들을 포함하고 있는 패키지 (https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        " - `torchvision.transforms`: 데이터 변형을 위한 라이브러리\n",
        "\n",
        " - `torch.utils.data.DataLoader`: 데이터를 `batch size` 크기로 분할해주는 라이브러리\n",
        "\n",
        "\n",
        "1) 딥러닝 모델 생성 관련 패키지\n",
        "\n",
        "  - `torch.nn (nn)`: 딥러닝 모델을 생성할 때 층(layer) 관련 함수들(예: Linear, Conv2d, MaxPool2d, ReLU, BatchNorm2d)을 모아놓은 패키지\n",
        "\n",
        "  - `torch.nn.Module (Module)`: 생성한 층들을 연합하여 하나의 모델로 구성할 때 사용하는 클래스\n",
        "\n",
        "2) 딥러닝 모델 학습 관련 패키지\n",
        "\n",
        "  - `torch.optim (optim)`: 모델 업데이트 알고리즘(`optimizer`)들을 모아놓은 패키지 "
      ],
      "metadata": {
        "id": "GUbz4izECgN1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn87b6T7c7Zj"
      },
      "source": [
        " ### Step 1) 데이터 불러오기 및 전처리 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIliloq9MD9H"
      },
      "source": [
        " - CIFAR10 데이터셋 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yRGukIoc7Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84a6423-bf0f-4715-c1bb-e9d104b6467d"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), \n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "     )\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"../data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"../data\", train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZwAXBMtnPL7"
      },
      "source": [
        " - **Data Augmentation (데이터 증강)**\n",
        "  \n",
        "  - 원본 데이터를 변형하여 데이터의 개수를 늘리는 기법 (**데이터 다양화**)\n",
        "      - 변형의 예) 회전(rotation), 반전 or 대칭(flip), 잘라내기(crop)\n",
        "\n",
        "      <img src=\"https://miro.medium.com/max/1400/0*LR1ZQucYW96prDte\" width=700>\n",
        "\n",
        "      - 참고) `ResNet` 학습에도 `Data Augmentation` 기법이 사용되었습니다! (https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "  - `Data Augmenetation`을 사용하면 성능을 향상시킬 수 있을까?\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_data%20augmentation.jpg' border='0'></a>\n",
        "\n",
        "    **Flip**\n",
        "\n",
        "    - 왼쪽만 바라보는 고양이가 train 때 주어졌다면 오른쪽을 보는 고양이는 test 때 맞추지 못할 수 있습니다.\n",
        "    \n",
        "    - 이 경우, 왼쪽만 바라보는 고양이를 좌우 대칭시켜 모두 학습에 사용하면 test 때 고양이가 어느 쪽을 보더라도 맞출 수 있게 됩니다.\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_data%20augmentation%202.jpg' border='0'></a>\n",
        "\n",
        "    **Crop**\n",
        "\n",
        "    - 딥러닝 모델이 꼬리와 귀, 수염, 눈이라는 특징으로 고양이인지를 판단한다면 고양이가 침대 밑에 숨어있어 꼬리만 보이는 경우 고양이라고 판단하지 못할 수 있습니다.\n",
        "\n",
        "    - 이 경우, 고양이의 각 부분을 잘 알려주기 위해 꼬리 부분만 잘라서 넣어준다면 성능이 더 좋아질 수 있을 것입니다.\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_%EB%B0%9D%EA%B8%B0%20%EC%A1%B0%EC%A0%88.jpg' border='0'></a>\n",
        "\n",
        "    **💡 밝기 조절**  \n",
        "    - 만약 딥러닝 모델로 앱을 만들게 되면, 사진을 찍는 사람마다 빛의 양이 다르겠죠? 딥러닝이 동일하게 인식할 수 있도록 어두운 것부터 밝은 것까지 모든 사진의 밝기를 조절해서 넣어줍시다!    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PyTorch에서는 `torchvision.transforms` 라이브러리를 통하여 `data augmentation`을 쉽게 구현할 수 있습니다!"
      ],
      "metadata": {
        "id": "bix_MqiglFJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        transforms.RandomCrop(size=32, padding=4),\n",
        "        transforms.RandomHorizontalFlip()\n",
        "    ]\n",
        ")\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)"
      ],
      "metadata": {
        "id": "0DYwoSymlgJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a14b67-dcd2-4077-fadb-be58f157d92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 뿐만 아니라, `DataLoader` 함수를 통하여 전체 데이터셋을 `배치 (batch)` 단위로 쪼갤 수 있습니다."
      ],
      "metadata": {
        "id": "WA0mtqadl3Mz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqMz8cFpPkaV"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x45v3oflpl1d"
      },
      "source": [
        "### Step 2) 인공신경망 생성\n",
        "\n",
        " - PyTorch에서는 `torch.nn` 라이브러리 내의 함수들과 `클래스 (class)` 용법을 사용하여 인공신경망을 생성합니다.\n",
        "\n",
        "    - **nn.Conv2d**: 합성곱 계층을 생성하는 함수\n",
        "\n",
        "    - **nn.MaxPool2d**: 풀링 계층을 생성하는 함수\n",
        "\n",
        "    - **nn.Flatten**: 다차원 데이터를 1차원으로 평탄화 시켜주는 함수\n",
        "\n",
        "    - **nn.Linear**: 전결합계층을 생성하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyOPgcNBPvqI"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=\"same\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=\"same\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"same\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=2048, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = nn.ReLU()(self.conv1(x))\n",
        "        pool1 = self.pool1(conv1)\n",
        "\n",
        "        conv2 = nn.ReLU()(self.conv2(pool1))\n",
        "        pool2 = self.pool2(conv2)\n",
        "\n",
        "        conv3 = nn.ReLU()(self.conv3(pool2))\n",
        "        pool3 = self.pool3(conv3)\n",
        "\n",
        "        flatten = self.flatten(pool3)\n",
        "\n",
        "        logits = self.fc1(flatten)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWTqGX4XNdY0"
      },
      "source": [
        " - 의도에 맞게 네트워크가 생성되었는지 확인해봅시다!\n",
        "\n",
        "    - **summary** 함수 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGpJJLNyDFJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ed205a-fc5f-4091-e67f-967fd8be5cbe"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = MyModel()\n",
        "model.to(\"cuda\")\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "         MaxPool2d-2           [-1, 32, 16, 16]               0\n",
            "            Conv2d-3           [-1, 64, 16, 16]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5            [-1, 128, 8, 8]          73,856\n",
            "         MaxPool2d-6            [-1, 128, 4, 4]               0\n",
            "           Flatten-7                 [-1, 2048]               0\n",
            "            Linear-8                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.56\n",
            "Params size (MB): 0.43\n",
            "Estimated Total Size (MB): 1.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-tYaD0quUI7"
      },
      "source": [
        "### Step 3) 학습 (Training)\n",
        " \n",
        " - 손실 함수 (Loss) 및 업데이트 알고리즘 (Optimizer) 설정\n",
        "    \n",
        "    - 손실함수로는 **교차엔트로피오차 (cross entropy)**를\n",
        "    \n",
        "    - 업데이트 알고리즘으로는 **adam** optimizer를 사용하겠습니다.\n",
        "\n",
        "        - `torch.optim (optim)`: 모델 업데이트 알고리즘(`optimizer`)들을 모아놓은 패키지 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btwwUUdFuaVS"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PyTorch에서는 GPU를 사용할 경우, **모델과 데이터를 모두 GPU에 올려주어야 합니다.**"
      ],
      "metadata": {
        "id": "nw_ICSpNoyBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8xDcs8CvovXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca106dbc-5d90-42e5-e8d5-f07d2944970d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmfGFWpNulDu"
      },
      "source": [
        " - 👨🏻‍🏫 학습 순서\n",
        "\n",
        "    1. 입력 (x)에 대한 결과 예측 -> y_pred\n",
        "\n",
        "    2. y_pred와 정답 (y_true)을 비교하여 손실함수의 값 계산 -> **criterion(y_pred, y_true)**\n",
        "\n",
        "    3. 손실함수의 값이 작아지는 방향으로 네트워크 업데이트 -> **optimizer.step()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSqvJyw9uhW5",
        "outputId": "7c75916e-89e3-4a29-e3d4-8194dc0ddda1"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    running_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 입력에 대한 결과 예측\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        # 예측값과 정답을 비교하여 손실함수의 값 계산\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "\n",
        "        # 손실함수의 값이 작아지는 방향으로 네트워크 업데이트\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            images, labels = batch\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}, Train Loss: {running_loss / total}, Train Accuracy: {100 * correct / total}')\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 0.04507628124833107, Train Accuracy: 57.25\n",
            "Test Accuracy: 60.36\n",
            "Epoch: 2, Train Loss: 0.03320949960827827, Train Accuracy: 66.852\n",
            "Test Accuracy: 69.42\n",
            "Epoch: 3, Train Loss: 0.028798986775279044, Train Accuracy: 69.17\n",
            "Test Accuracy: 71.48\n",
            "Epoch: 4, Train Loss: 0.02636519924044609, Train Accuracy: 72.058\n",
            "Test Accuracy: 73.15\n",
            "Epoch: 5, Train Loss: 0.0246919292140007, Train Accuracy: 74.768\n",
            "Test Accuracy: 75.06\n",
            "Epoch: 6, Train Loss: 0.023364896874427797, Train Accuracy: 75.782\n",
            "Test Accuracy: 75.76\n",
            "Epoch: 7, Train Loss: 0.022320393640995025, Train Accuracy: 76.148\n",
            "Test Accuracy: 77.32\n",
            "Epoch: 8, Train Loss: 0.02160057497948408, Train Accuracy: 77.264\n",
            "Test Accuracy: 77.53\n",
            "Epoch: 9, Train Loss: 0.02088353258550167, Train Accuracy: 78.264\n",
            "Test Accuracy: 78.16\n",
            "Epoch: 10, Train Loss: 0.02063079702347517, Train Accuracy: 77.912\n",
            "Test Accuracy: 77.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2) **배치 정규화(Batch Normalization)**"
      ],
      "metadata": {
        "id": "f4yu17XK27CM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `Gradient Vanishing Problem`: `Backpropagation`에 의해 gradient가 앞단으로 전파되면서 점점 옅어지게 되어 너무 작아져서 **소멸**하게 되는 문제\n",
        "\n",
        "    <img src=\"https://t1.daumcdn.net/cfile/tistory/997E1B4C5BB6EAF239\" width=700>\n",
        "\n",
        "    - Layer를 통과할 때마다 `활성함수의 미분값(gradient)`이 계속 곱해지는데, 기울기가 완만한 영역의 입력값이 주어지는 경우 앞단의 `gradient`가 아주 작아 모델이 업데이트 되지 않는 문제"
      ],
      "metadata": {
        "id": "536TWaamvMjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `Internal Covariate Shift`: Layer를 통과할 때마다 입력값의 분포가 변화하는 현상\n",
        "\n",
        "    <img src='https://datadiving.dothome.co.kr/Deep%204_internal%20Covariate%20Shift%202.jpg' width=700>\n",
        "\n",
        "    - Layer가 깊어질수록 이전에 더 많은 layer를 통과하기 때문에 입력값의 분포가 많이 변하게 된다.\n",
        "\n",
        "    <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/3.png\" width=700>\n",
        "\n",
        "    - 입력값의 분포가 점점 변형되어 `gradient`가 작은 영역에 도달하게 된다면 `vanishing gradient`문제가 발생한다."
      ],
      "metadata": {
        "id": "6TTMpW6ctz3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/4.png\" width=700>\n",
        "\n",
        "\n",
        " - `배치 정규화 (Batch Normalization)`: 각 레이어를 통과할 때마다 **정규화 (normalization)**하는 레이어를 두어 입력값의 분포가 변형되지 않도록 조절하는 알고리즘\n",
        "\n",
        "  - 배치 단위로 실시하기 때문에 `batch normalization`이라고 부릅니다.\n",
        "\n",
        "  - 활성함수의 입력값의 분포를 조절해주는 것이 목적이므로 **활성함수** 앞에 넣어준다.\n",
        "\n",
        "  - 알고리즘 테이블\n",
        "\n",
        "    <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcFYkLE%2FbtqEcUnlXKy%2FZbGZNjObjo2gL2xss8zYzk%2Fimg.png\" width=500>\n",
        "\n",
        "  - $\\gamma, \\beta$로 `scale and shift`를 해주는 이유?\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/1400/1*0yhJ7DbhOX-tRUseljjYoA.png\" width=700>"
      ],
      "metadata": {
        "id": "HV58Sr6HuKnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **VGG16**에 `BatchNormalization`을 추가한 모델을 PyTorch를 이용하여 생성해봅시다.\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/857/1*AqqArOvacibWqeulyP_-8Q.png\" width=700>"
      ],
      "metadata": {
        "id": "qq7hIl2t_fAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MyModel(input_shape, output_dim, num_filters_list, use_batch_norm):\n",
        "  # 입력계층 (Input Layer)\n",
        "  img = layers.Input(shape=input_shape) # cifar10의 경우는 input_shape=[32, 32, 3]\n",
        "\n",
        "  # 특징 추출 파트 - CNN\n",
        "  h = img\n",
        "  for num_filters in num_filters_list:\n",
        "    if num_filters == \"max_pool\": \n",
        "      h = layers.MaxPooling2D(pool_size=(2, 2))(h)\n",
        "    else:\n",
        "      h = layers.Conv2D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\")(h) # convolution \n",
        "      if use_batch_norm == True:\n",
        "        h = layers.BatchNormalization()(h) # batch normalization\n",
        "      h = layers.ReLU()(h) # activation\n",
        "\n",
        "  # 분류 파트 - MLP\n",
        "  mlp_input = layers.Flatten()(h)\n",
        "  prob = layers.Dense(units=output_dim, activation=\"softmax\")(mlp_input)\n",
        "\n",
        "  # 전체 모델\n",
        "  return Model(inputs=img, outputs=prob)"
      ],
      "metadata": {
        "id": "Ea2_0wHNlQCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SQEdnDltFTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3) **ResNet** 구현\n",
        "\n",
        " - ResNet에서의 key idea는 2개의 layer를 통과할 때마다 `skip connection`을 넣어주어`backpropagation` 과정에서 **gradient가 잘 흐르도록 하는 것**입니다.\n",
        " \n",
        " <img src=\"https://images.velog.io/images/junyoung9696/post/3137e50c-b52f-4cdd-8ae8-2faf497efe84/r10.png\" width=500>\n",
        "\n",
        " - ResNet 논문(https://arxiv.org/pdf/1512.03385.pdf)을 참고하여 CIFAR10 이미지 분류 네트워크를 생성해봅시다.\n",
        "\n",
        "  <img src=\"https://blog.kakaocdn.net/dn/bQfaUX/btqYAtD1KcX/Zdc4DLFzR9SoJYBlO6M1uK/img.png\" width=700>"
      ],
      "metadata": {
        "id": "UfXBunC6Ow_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Step 1) Residual Block 구성"
      ],
      "metadata": {
        "id": "mHLLZU95xCSt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_KSlHjXwo14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}