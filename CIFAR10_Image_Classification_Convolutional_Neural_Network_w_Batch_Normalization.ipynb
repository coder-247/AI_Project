{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEWH1nAG_v4Q"
      },
      "source": [
        "# Part 1) CNN을 이용한 CIFAR10 이미지 분류\n",
        "\n",
        "## Step 0) **딥러닝 패키지 import**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbGSKBPZc7Zh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `tensorflow.keras.layers (layers)`: 딥러닝 네트워크를 설계할 때 층(layer) 관련 함수들(예:`Dense`, `Conv2D`, `MaxPooling2D`, `SimpleRNN`, `LSTM`)을 모아놓은 라이브러리\n",
        "\n",
        " - `tensorflow.keras.models.Model (Model)`: 생성한 층들을 연합하여 하나의 모델로 구성할 때 사용하는 함수\n",
        "\n",
        " - `tensorflow.keras.datasets`: TensorFlow에서 딥러닝 실습을 위해 제공해주는 데이터셋 (예: `mnist`, `cifar10`, `cifar100`, `imdb`)\n",
        "\n",
        " - `numpy (np)`: 다차원 데이터 처리를 위한 라이브러리 (참고: `pandas`-2차원 데이터에 특화된 라이브러리)\n",
        "\n",
        " - `matplotlib.pyplot (plt)`: 데이터 시각화를 위한 라이브러리"
      ],
      "metadata": {
        "id": "hGfyQNsd6FAB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn87b6T7c7Zj"
      },
      "source": [
        " ## Step 1-1) 데이터 불러오기\n",
        "\n",
        " <img src=\"https://user-images.githubusercontent.com/15958325/63308580-41b7fe80-c32e-11e9-827f-98052675c0ea.png\" width=\"800\">\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yRGukIoc7Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652a5f5e-ea24-4a2e-ff59-06b9441dfa48"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMf8IYuLBype"
      },
      "source": [
        "## Step 0) 데이터 전처리\n",
        "\n",
        " - CNN은 이미지 처리에 특화된 인공신경망이기 때문에 **3차원 데이터를 입력**으로 받도록 설계 되어있습니다. \n",
        "  - MLP에서처럼 이미지를 1차원으로 **reshape할 필요가 없습니다!**\n",
        " \n",
        " - 단, 학습의 안정성을 위해 각 픽셀의 값이 0부터 1 사이가 되도록 정규화(normalization)는 합니다.\n",
        "\n",
        " - Category의 갯수가 3개 이상인 범주형 데이터에 대해서는 **tf.one_hot** 함수를 이용하여 **one-hot encoding**을 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZjvKmyXB-GW"
      },
      "source": [
        "x_train_cnn = x_train / 255\n",
        "x_test_cnn = x_test / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLia549WCAGL"
      },
      "source": [
        "y_train_onehot = tf.squeeze(tf.one_hot(y_train, 10))\n",
        "y_test_onehot = tf.squeeze(tf.one_hot(y_test, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt17Uz2tCAwB"
      },
      "source": [
        "## Step 1) 네트워크 구조 설계\n",
        "\n",
        " - `합성곱(convolution)` 연산\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_6.webp\" width=500>\n",
        " \n",
        " - `Convolution`의 기능\n",
        "  - `Convolution`은 `filter (kernel)`을 통하여 특정 **특징(feature)의 유무 및 위치**를 식별하는데 특화된 연산입니다.\n",
        "  - 이러한 이유로 이미지 분류 (image classification) 외에도 `객체 위치 식별 (object localization)`, `객체 탐지 (object detection)`, `이미지 분할 (image segmentation)`과 같은 많은 어플리케이션에서 CNN이 사용되었습니다.\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep%203_7.jpg' width=700>\n",
        " \n",
        " - CNN에 기반한 이미지 분류 네트워크는 \n",
        "  - 1) 이미지의 **특징을 추출**하는 **CNN** 파트와 (`Conv2D`, `MaxPooling2D` 이용)\n",
        "    - **추출하는 특징을 점점 구체화(예: 테두리 -> 눈,코,귀 -> 얼굴)**하기 위하여 **Conv layer 중간 중간에 Pooling layer 추가**\n",
        "    - 일반적으로 특징이 점점 구체화 될수록 많은 수의 특징 추출, **filter의 갯수 증가**\n",
        "    - 많은 수의 Conv layer를 사용하기 위하여 **zero padding** 사용\n",
        "      - **Conv layer를 통과했을 때 이미지의 크기가 줄어드는 것 방지**\n",
        "\n",
        "  - 2) **추출된 특징을 이용하여 classification을 수행**하는 **MLP** 파트로 나뉩니다.\n",
        "    - CNN의 출력은 3차원이기 때문에 MLP에 넣어줄 때 **1차원으로 reshape** (`Flatten` 이용) 해주어야 합니다.\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_4.png\" width=1100> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `layers.Conv2D`\n",
        " - `filters`: 총 몇개의 filter를 사용할 것인가 (총 몇개의 특징을 추출할 것인가)\n",
        " - `kernel_size`: filter의 size (보통 3*3을 사용)\n",
        " - `strides`: 보폭 (보통 1)\n",
        " - `padding`: conv layer를 거쳐도 이미지의 크기가 줄어들지 않도록 테두리 부분에 0을 집어넣는 것. (padding=\"same\")\n",
        " - `activation`: 활성함수, `relu`를 주로 사용\n",
        "\n",
        "- `layers.MaxPooling2D`\n",
        " - `pool_size`: 하나의 값으로 종합하고 싶은 영역 (보통 2*2를 사용)\n",
        "\n",
        "- `layers.Flatten`"
      ],
      "metadata": {
        "id": "Xh6EqeLmly3r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llid0guYJSZA"
      },
      "source": [
        "## 입력계층\n",
        "img = layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "## 특징 추출 파트 - CNN\n",
        "# conv + pool 계속 반복되는 구조\n",
        "conv1 = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(img)  # [32, 32, 16]\n",
        "pool1 = layers.MaxPooling2D(pool_size=2)(conv1) # [16, 16, 16]\n",
        "\n",
        "conv2 = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(pool1) # [16, 16, 32]\n",
        "pool2 = layers.MaxPooling2D(pool_size=2)(conv2) # [8, 8, 32]\n",
        "\n",
        "conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(pool2) # [8, 8, 64]\n",
        "pool3 = layers.MaxPooling2D(pool_size=2)(conv3) # [4, 4, 64]\n",
        "\n",
        "conv4 = layers.Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(pool3) # [4, 4, 128]\n",
        "pool4 = layers.MaxPooling2D(pool_size=2)(conv4) # [2, 2, 128]\n",
        " \n",
        "## 분류 파트 - MLP\n",
        "mlp_input = layers.Flatten()(pool4) # 2*2*128\n",
        "prob = layers.Dense(units=10, activation=\"softmax\")(mlp_input)\n",
        "\n",
        "## 전체 모델\n",
        "model = Model(inputs=img, outputs=prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGU_-H2BC1a"
      },
      "source": [
        " - MLP에서와 마찬가지로 `conv layer`의 수와 `filter (feature map)`의 수는 **실험적**으로 결정되기 때문에 여러 개의 모델을 생성 및 학습하여 성능을 비교해보셔야 합니다.\n",
        "\n",
        " - 다음 세 가지 값을 입력으로 받아 그에 해당하는 합성곱 신경망을 생성해주는 함수를 만들어봅시다:\n",
        "   - 입력 데이터 모양 `input_shape`\n",
        "   - 결과 데이터 차원 `output_dim`\n",
        "   - `conv layer`의 `filter` 수를 모아놓은 `num_filters_list` (예: num_filters_list = [16, 'max_pool', 32, 'max_pool', 64, 'max_pool', 128, 'max_pool'])\n",
        "    - 'max_pool'인 경우 `pooling`을 적용하는 것으로 생각해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l50498-CBDmP"
      },
      "source": [
        "def MyModel(input_shape, output_dim, num_filters_list):\n",
        "  # 입력계층 (Input Layer)\n",
        "  img = layers.Input(shape=input_shape) # cifar10의 경우는 input_shape=[32, 32, 3]\n",
        "\n",
        "  # 특징 추출 파트 - CNN\n",
        "  h = img\n",
        "  for num_filters in num_filters_list:\n",
        "    if num_filters == \"max_pool\": # == -> 같다, = -> 할당\n",
        "      h = layers.MaxPooling2D(pool_size=(2, 2))(h)\n",
        "    else:\n",
        "      h = layers.Conv2D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(h)\n",
        "\n",
        "  # 분류 파트 - MLP\n",
        "  mlp_input = layers.Flatten()(h)\n",
        "  prob = layers.Dense(units=output_dim, activation=\"softmax\")(mlp_input)\n",
        "\n",
        "  # 전체 모델\n",
        "  return Model(inputs=img, outputs=prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `num_filters_list=[16, 'max_pool', 32, 'max_pool', 64, 'max_pool', 128, 'max_pool']`인 경우에 네트워크를 생성해봅시다."
      ],
      "metadata": {
        "id": "Gq1IOJidKD77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(input_shape=(32, 32, 3), output_dim=10, num_filters_list=[16, 'max_pool', 32, 'max_pool', 64, 'max_pool', 128, 'max_pool'])"
      ],
      "metadata": {
        "id": "GmgtcyyBKPzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJs4HTfCsj1"
      },
      "source": [
        "- `summary` 함수를 이용하여 딥러닝 네트워크가 의도에 맞게 설계되었는지 확인하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fbH0oeWx3dNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d180e87a-dffb-4744-cc0e-93e090dd9167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 32, 32, 16)        448       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 16, 16, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 8, 8, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 4, 4, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 2, 2, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 102,570\n",
            "Trainable params: 102,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `batch_size=32`, `epochs=15`로 설정하여 생성한 모델을 학습시켜봅시다."
      ],
      "metadata": {
        "id": "xTV-2xony74g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수(loss), 모델 업데이트 알고리즘(optimizer), 평가지표(metrics) 정의\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 학습\n",
        "model.fit(x=x_train_cnn, y=y_train_onehot, batch_size=32, epochs=15, verbose=1, validation_data=(x_test_cnn, y_test_onehot))"
      ],
      "metadata": {
        "id": "nQXqPA_vzC_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9c56fc-1024-4af1-c247-e81835bbecef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4643 - accuracy: 0.4692 - val_loss: 1.1608 - val_accuracy: 0.5878\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0790 - accuracy: 0.6193 - val_loss: 1.0253 - val_accuracy: 0.6433\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9139 - accuracy: 0.6812 - val_loss: 0.9421 - val_accuracy: 0.6671\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8005 - accuracy: 0.7218 - val_loss: 0.8603 - val_accuracy: 0.7002\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7069 - accuracy: 0.7536 - val_loss: 0.8617 - val_accuracy: 0.7066\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6353 - accuracy: 0.7779 - val_loss: 0.8367 - val_accuracy: 0.7171\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5728 - accuracy: 0.8000 - val_loss: 0.8410 - val_accuracy: 0.7187\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5109 - accuracy: 0.8213 - val_loss: 0.9167 - val_accuracy: 0.7056\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4662 - accuracy: 0.8361 - val_loss: 0.9613 - val_accuracy: 0.7057\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4179 - accuracy: 0.8528 - val_loss: 0.9278 - val_accuracy: 0.7196\n",
            "Epoch 11/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3741 - accuracy: 0.8661 - val_loss: 1.0247 - val_accuracy: 0.7074\n",
            "Epoch 12/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3395 - accuracy: 0.8786 - val_loss: 1.0598 - val_accuracy: 0.7156\n",
            "Epoch 13/15\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3066 - accuracy: 0.8908 - val_loss: 1.0925 - val_accuracy: 0.7133\n",
            "Epoch 14/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2709 - accuracy: 0.9031 - val_loss: 1.2268 - val_accuracy: 0.6993\n",
            "Epoch 15/15\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2547 - accuracy: 0.9094 - val_loss: 1.2701 - val_accuracy: 0.7004\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbc001aaa50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Part 2) **배치 정규화 (Batch Normalization)**와 **잔차 학습 (Residual Connection)**"
      ],
      "metadata": {
        "id": "qutV2uR10Knd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://datadiving.dothome.co.kr/Deep%204_2.png' border='0'></a>"
      ],
      "metadata": {
        "id": "pUFZ7Lh61-xP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pWCMRfbJ05o"
      },
      "source": [
        "- **ImageNet Challenge**\n",
        "    - Image classification 알고리즘의 성능을 평가하는 대회\n",
        "    - 120만개의 학습 데이터 (training data), 5만개의 검증 데이터 (validation data), 10만개의 테스트 데이터 (test data) \n",
        "     - 참고: https://image-net.org/download.php\n",
        "    - 1000개의 클래스\n",
        "    - 대회에서 우승한 알고리즘들은 모두 컴퓨터 비전 분야 발전에 큰 역할을 해왔습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://datadiving.dothome.co.kr/Deep%203_3.webp\" width=800>\n",
        "\n",
        "  - 2012년 `AlexNet` 이후에는 **CNN 기반 모델이 우승**하였고, 2015년도에는 **사람보다 뛰어난** `ResNet`이 개발되었습니다."
      ],
      "metadata": {
        "id": "K4OTKtwY2HHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - 우승한 모델의 깊이(depth)가 점점 깊어지는 것을 확인하실 수 있습니다.\n",
        "\n",
        "    - **VGG16** (2014 ImageNet Challenge Winner; 16 layers)\n",
        "  <img src=\"https://miro.medium.com/max/857/1*AqqArOvacibWqeulyP_-8Q.png\">\n",
        "\n",
        "    - **ResNet** (2015 ImageNet Challenge Winner; upto 152 layers)\n",
        "    <img src=\"https://blog.kakaocdn.net/dn/bQfaUX/btqYAtD1KcX/Zdc4DLFzR9SoJYBlO6M1uK/img.png\">\n",
        "  \n",
        "  - **VGG16**을 모방하여 `num_filters_list=[64, 64, 'max_pool', 128, 128, 'max_pool', 256, 256, 256, 'max_pool', 512, 512, 512, 'max_pool']`에 해당하는 네트워크를 생성해봅시다."
      ],
      "metadata": {
        "id": "-oFBZixRKd2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델 생성\n",
        "vgg = MyModel(input_shape=(32, 32, 3), output_dim=10, num_filters_list=[64, 64, 'max_pool', 128, 128, 'max_pool', 256, 256, 256, 'max_pool', 512, 512, 512, 'max_pool'])\n",
        "\n",
        "## 모델 출력\n",
        "vgg.summary()"
      ],
      "metadata": {
        "id": "9w9LjyKuMkqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `batch_size=32`, `epochs=5`로 설정하여 간단히 학습시켜봅시다."
      ],
      "metadata": {
        "id": "EFuM9ZUd3ufw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 손실함수(loss), 모델 업데이트 알고리즘(optimizer), 평가지표(metrics) 설정\n",
        "vgg.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "## 학습\n",
        "vgg.fit(x=x_train_cnn, y=y_train_onehot, batch_size=32, epochs=5, verbose=1, validation_data=(x_test_cnn, y_test_onehot))"
      ],
      "metadata": {
        "id": "x54FJJFP3j_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bb0f9b-d506-4e36-a3e9-f49edd99f721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 33s 20ms/step - loss: 2.3028 - accuracy: 0.0971 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3028 - accuracy: 0.0998 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3028 - accuracy: 0.0985 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3028 - accuracy: 0.0979 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3028 - accuracy: 0.0993 - val_loss: 2.3026 - val_accuracy: 0.1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbbcc1fb590>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **학습이 전혀 안 되고 있습니다...!** 왜 그럴까요...?\n",
        "\n",
        " - `역전파(Backpropagation)`: 미분의 연쇄법칙(chain rule)에 근거하여 인공신경망의 가중치와 편차를 업데이트하는 알고리즘.\n",
        "\n",
        "<img src=\"https://goldenrabbit.co.kr/wp-content/uploads/2022/08/3-1.jpg\" width=900>\n",
        "\n",
        " - `Gradient Vanishing Problem`: `Backpropagation`에 의해 gradient가 앞단으로 전파되면서 점점 옅어지게 되어 너무 작아져서 **소멸**하게 되는 문제\n",
        "\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/997E1B4C5BB6EAF239\" width=700>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*0yhJ7DbhOX-tRUseljjYoA.png\" width=700>\n",
        "\n",
        " - `Internal Covariate Shift`\n",
        " <img src='https://datadiving.dothome.co.kr/Deep%204_internal%20Covariate%20Shift%202.jpg' width=700>\n",
        "\n",
        " <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/3.png\" width=700>\n",
        "\n",
        " - `배치 정규화 (Batch Normalization)`: 각 레이어를 통과할 때마다 **정규화 (normalization)**하는 레이어를 두어 분포가 변형되지 않도록 조절하는 메커니즘\n",
        "\n",
        "  - 배치 단위로 실시하기 때문에 `batch normalization`이라고 부릅니다.\n",
        "\n",
        " <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/4.png\" width=700>\n",
        "\n",
        "\n",
        " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcFYkLE%2FbtqEcUnlXKy%2FZbGZNjObjo2gL2xss8zYzk%2Fimg.png\" width=500>"
      ],
      "metadata": {
        "id": "UfXBunC6Ow_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 입력계층에서는 데이터 전처리를 통해 `예쁜 데이터`를 넣어준다 하더라도, 레이어를 거듭할수록 데이터의 분포가 변화하게 됨. \n",
        " \n",
        " - 운이 나쁘게 음수만 되도록 변화하면, gradient=0이 되어 학습이 되지 않는 문제가 발생\n",
        "\n",
        " - 나쁜 분포의 데이터가 relu를 통과하면 학습이 잘 일어나지 않는다. -> **relu를 통과시키기 전**에 분포를 예쁘게 만들어준다 (batch normalization을 적용해준다)! "
      ],
      "metadata": {
        "id": "1xEygF9tgw_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **VGG16**에 `BatchNormalization`을 추가한 네트워크를 생성해봅시다."
      ],
      "metadata": {
        "id": "qq7hIl2t_fAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MyModel(input_shape, output_dim, num_filters_list):\n",
        "  # 입력계층 (Input Layer)\n",
        "  img = layers.Input(shape=input_shape) # cifar10의 경우는 input_shape=[32, 32, 3]\n",
        "\n",
        "  # 특징 추출 파트 - CNN\n",
        "  h = img\n",
        "  for num_filters in num_filters_list:\n",
        "    if num_filters == \"max_pool\": # == -> 같다, = -> 할당\n",
        "      h = layers.MaxPooling2D(pool_size=(2, 2))(h)\n",
        "    else:\n",
        "      # convolution -> activation\n",
        "      # convolution -> batch normalization -> activation\n",
        "      h = layers.Conv2D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\")(h) # convolution\n",
        "      h = layers.BatchNormalization()(h) # batch normalization\n",
        "      h = layers.ReLU()(h) # activation\n",
        "\n",
        "  # 분류 파트 - MLP\n",
        "  mlp_input = layers.Flatten()(h)\n",
        "  prob = layers.Dense(units=output_dim, activation=\"softmax\")(mlp_input)\n",
        "\n",
        "  # 전체 모델\n",
        "  return Model(inputs=img, outputs=prob)"
      ],
      "metadata": {
        "id": "FaRUGySWVCf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MyModel(input_shape, output_dim, num_filters_list, use_batch_norm):\n",
        "  # 입력계층 (Input Layer)\n",
        "  img = layers.Input(shape=input_shape) # cifar10의 경우는 input_shape=[32, 32, 3]\n",
        "\n",
        "  # 특징 추출 파트 - CNN\n",
        "  h = img\n",
        "  for num_filters in num_filters_list:\n",
        "    if num_filters == \"max_pool\": # == -> 같다, = -> 할당\n",
        "      h = layers.MaxPooling2D(pool_size=(2, 2))(h)\n",
        "    else:\n",
        "      # convolution -> activation\n",
        "      # convolution -> batch normalization -> activation\n",
        "      h = layers.Conv2D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\")(h) # convolution\n",
        "      if use_batch_norm == True:\n",
        "        h = layers.BatchNormalization()(h) # batch normalization\n",
        "      h = layers.ReLU()(h) # activation\n",
        "\n",
        "  # 분류 파트 - MLP\n",
        "  mlp_input = layers.Flatten()(h)\n",
        "  prob = layers.Dense(units=output_dim, activation=\"softmax\")(mlp_input)\n",
        "\n",
        "  # 전체 모델\n",
        "  return Model(inputs=img, outputs=prob)"
      ],
      "metadata": {
        "id": "Ea2_0wHNlQCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(input_shape=(32, 32, 3), output_dim=10, num_filters_list=[64, 64, 'max_pool', 128, 128, 'max_pool', 256, 256, 256, 'max_pool', 512, 512, 512, 'max_pool'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qabNQLHX_rb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabbc848-864d-4ff2-8120-977aa41c4b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 16, 16, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 8, 8, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 8, 8, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_6 (ReLU)              (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 4, 4, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4, 4, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_7 (ReLU)              (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4, 4, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_8 (ReLU)              (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 4, 4, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_9 (ReLU)              (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 2, 2, 512)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,666,506\n",
            "Trainable params: 7,661,130\n",
            "Non-trainable params: 5,376\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - `batch_size=32`, `epochs=30`으로 설정하여 생성한 모델을 학습시켜봅시다."
      ],
      "metadata": {
        "id": "iqgOhjqs_zcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train_cnn, y_train_onehot, batch_size=32, epochs=30, verbose=1, validation_data=[x_test_cnn, y_test_onehot])"
      ],
      "metadata": {
        "id": "iOG0DJk-Vicn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a329b09-8253-430b-ca90-c8b5dddcb7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 36s 22ms/step - loss: 1.6284 - accuracy: 0.4203 - val_loss: 1.5531 - val_accuracy: 0.4937\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.9541 - accuracy: 0.6682 - val_loss: 1.0486 - val_accuracy: 0.6502\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.7223 - accuracy: 0.7533 - val_loss: 1.0954 - val_accuracy: 0.6628\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.6008 - accuracy: 0.7947 - val_loss: 0.7124 - val_accuracy: 0.7635\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.4969 - accuracy: 0.8318 - val_loss: 0.7029 - val_accuracy: 0.7671\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4031 - accuracy: 0.8630 - val_loss: 0.7336 - val_accuracy: 0.7571\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.3322 - accuracy: 0.8851 - val_loss: 0.5781 - val_accuracy: 0.8115\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.2733 - accuracy: 0.9069 - val_loss: 0.5302 - val_accuracy: 0.8298\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.2156 - accuracy: 0.9252 - val_loss: 0.7040 - val_accuracy: 0.7977\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1699 - accuracy: 0.9423 - val_loss: 0.6008 - val_accuracy: 0.8179\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1379 - accuracy: 0.9521 - val_loss: 0.7409 - val_accuracy: 0.8075\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1127 - accuracy: 0.9604 - val_loss: 0.5650 - val_accuracy: 0.8468\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0959 - accuracy: 0.9658 - val_loss: 0.6218 - val_accuracy: 0.8437\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0838 - accuracy: 0.9712 - val_loss: 0.7843 - val_accuracy: 0.8192\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0702 - accuracy: 0.9763 - val_loss: 0.6936 - val_accuracy: 0.8375\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0642 - accuracy: 0.9780 - val_loss: 0.7973 - val_accuracy: 0.8294\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0613 - accuracy: 0.9790 - val_loss: 0.6983 - val_accuracy: 0.8473\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0511 - accuracy: 0.9830 - val_loss: 0.8070 - val_accuracy: 0.8328\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0511 - accuracy: 0.9828 - val_loss: 0.8528 - val_accuracy: 0.8142\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0463 - accuracy: 0.9841 - val_loss: 0.8592 - val_accuracy: 0.8327\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0409 - accuracy: 0.9862 - val_loss: 0.6705 - val_accuracy: 0.8617\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0415 - accuracy: 0.9859 - val_loss: 0.6926 - val_accuracy: 0.8507\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0370 - accuracy: 0.9874 - val_loss: 0.7487 - val_accuracy: 0.8492\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.7394 - val_accuracy: 0.8382\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0323 - accuracy: 0.9887 - val_loss: 0.8262 - val_accuracy: 0.8318\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0328 - accuracy: 0.9890 - val_loss: 0.7697 - val_accuracy: 0.8510\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0305 - accuracy: 0.9900 - val_loss: 0.8578 - val_accuracy: 0.8375\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0301 - accuracy: 0.9901 - val_loss: 0.7392 - val_accuracy: 0.8510\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.8287 - val_accuracy: 0.8443\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0271 - accuracy: 0.9909 - val_loss: 0.7452 - val_accuracy: 0.8604\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbba7731250>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train_cnn, y_train_onehot, batch_size=32, epochs=30, verbose=1, validation_data=[x_test_cnn, y_test_onehot])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K68U48kSVpbf",
        "outputId": "8de45d39-2797-4ee0-9ed6-a17afa4db032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 49s 22ms/step - loss: 1.4106 - accuracy: 0.4946 - val_loss: 1.4543 - val_accuracy: 0.5539\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.8613 - accuracy: 0.7030 - val_loss: 0.9138 - val_accuracy: 0.6924\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.6724 - accuracy: 0.7721 - val_loss: 0.8301 - val_accuracy: 0.7174\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.5504 - accuracy: 0.8129 - val_loss: 0.9234 - val_accuracy: 0.7190\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.4608 - accuracy: 0.8446 - val_loss: 0.5828 - val_accuracy: 0.8100\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.3793 - accuracy: 0.8704 - val_loss: 0.6074 - val_accuracy: 0.7949\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.3046 - accuracy: 0.8975 - val_loss: 0.7163 - val_accuracy: 0.7827\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.2417 - accuracy: 0.9160 - val_loss: 0.7558 - val_accuracy: 0.7798\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1988 - accuracy: 0.9314 - val_loss: 0.6951 - val_accuracy: 0.8014\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1545 - accuracy: 0.9465 - val_loss: 0.5979 - val_accuracy: 0.8350\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1277 - accuracy: 0.9562 - val_loss: 0.6643 - val_accuracy: 0.8332\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1104 - accuracy: 0.9628 - val_loss: 0.5908 - val_accuracy: 0.8412\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0906 - accuracy: 0.9694 - val_loss: 0.6821 - val_accuracy: 0.8346\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0802 - accuracy: 0.9722 - val_loss: 0.6977 - val_accuracy: 0.8356\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0716 - accuracy: 0.9754 - val_loss: 0.6744 - val_accuracy: 0.8470\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0665 - accuracy: 0.9767 - val_loss: 0.7303 - val_accuracy: 0.8411\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0581 - accuracy: 0.9805 - val_loss: 0.8052 - val_accuracy: 0.8381\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0596 - accuracy: 0.9793 - val_loss: 0.7215 - val_accuracy: 0.8450\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0506 - accuracy: 0.9830 - val_loss: 0.7686 - val_accuracy: 0.8430\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0492 - accuracy: 0.9828 - val_loss: 0.7415 - val_accuracy: 0.8460\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0459 - accuracy: 0.9850 - val_loss: 0.7732 - val_accuracy: 0.8465\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0445 - accuracy: 0.9856 - val_loss: 0.7152 - val_accuracy: 0.8531\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0405 - accuracy: 0.9865 - val_loss: 0.7866 - val_accuracy: 0.8489\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0388 - accuracy: 0.9868 - val_loss: 0.7499 - val_accuracy: 0.8529\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0363 - accuracy: 0.9882 - val_loss: 0.8853 - val_accuracy: 0.8229\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0357 - accuracy: 0.9882 - val_loss: 0.9177 - val_accuracy: 0.8333\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0343 - accuracy: 0.9892 - val_loss: 0.7912 - val_accuracy: 0.8427\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0304 - accuracy: 0.9900 - val_loss: 0.9607 - val_accuracy: 0.8249\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 0.7792 - val_accuracy: 0.8556\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0300 - accuracy: 0.9902 - val_loss: 0.8256 - val_accuracy: 0.8501\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f561485e5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}